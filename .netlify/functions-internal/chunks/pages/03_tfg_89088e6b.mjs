import { b as createComponent, s as spreadAttributes, r as renderTemplate, f as renderComponent, u as unescapeHTML } from '../astro_442ab39c.mjs';
import { $ as $$ProjectsLayout } from './01_coins_4a1e200e.mjs';
import '@astrojs/internal-helpers/path';
import 'html-escaper';
/* empty css                              *//* empty css                              */import 'nanostores';

const images = {
					
				};

				function updateImageReferences(html) {
					return html.replaceAll(
						/__ASTRO_IMAGE_="([^"]+)"/gm,
						(full, imagePath) => spreadAttributes({src: images[imagePath].src, ...images[imagePath].attributes})
					);
				}

				const html = updateImageReferences("<h2 class=\"projectTitles\">Index</h2>\n<ul>\n  <li> ‚¶ø <a href=\"#generalExplanation\">What's all this about? üò∂</a></li>\n  <li> ‚¶ø <a href=\"#techniques\">Techniques analyzed</a></li>\n  <li> ‚¶ø <a href=\"#dataset\">Dataset</a></li>\n  <li> ‚¶ø <a href=\"#baseModel\">Choosing the base model</a></li>\n    <li class=\"subList\"> ‚¶æ <a href=\"#transferLearning\">Transfer Learning</a></li>\n    <li class=\"subList\"> ‚¶æ <a href=\"#fineTuning\">Fine tuning</a></li>\n    <li class=\"subList\"> ‚¶æ <a href=\"#custom\">Customized CNN</a></li>\n    <li class=\"subList\"> ‚¶æ <a href=\"#baseResults\">Comparing base results</a></li>\n  <li> ‚¶ø <a href=\"#dataAugmentation\">Data augmentation</a></li>\n  <li> ‚¶ø <a href=\"#labelSmoothing\">Label smoothing</a></li>\n  <li> ‚¶ø <a href=\"#tta\">Test time augmentation</a></li>\n  <li> ‚¶ø <a href=\"#implementation\">Implementation  üë®üèª‚Äçüíª</a></li>\n</ul>\n<h2 id=\"generalExplanation\">What's all this about? üò∂</h2>\n<div><p><b>FER 2013</b> (Face Emotion Recognition) is a dataset that offers the possibility to study a vast number of facial expressions and predict the emotion they are feeling. This project's goal is to analyze the influence of certain machine learning and computer vision techniques.</p><br><p>Starting by creating a very simple <b>CNN</b> which classifies each picture and then applying little changes that almost doesn't mutate the model's structure, yet provide a considerable improvement in the classificator understanding of the problem.</p></div>\n<h2 id=\"techniques\">Techniques analyzed</h2>\n<ul>\n  <li> ‚¶ø Transfer learning</li>\n  <li> ‚¶ø Fine tuning</li>\n  <li> ‚¶ø Data augmentation</li>\n  <li> ‚¶ø Label smoothing</li>\n  <li> ‚¶ø Test time augmentation</li>\n</ul>\n<h2 id=\"dataset\">Dataset</h2>\n<div>\n  <p>All the images are 48x48, in grayscale, the faces are centered similarly and there are seven classes: angry, disgust, fear, happy, sad, surprise and neutral. 28708 examples are to train and 7178 to test.</p>\n  <p>As you can see in the sample emotions are ambiguous due to the fact that a face can be classified as more than one emotion and still be correct. Have we got better labels, the model would be much more accurate. To solve this problem I have used besides FER 2013 labels FER 2013+ ones, whom have been made by 10 persons each one voting a class.</p>\n  <img src=\"/imgs/fer+.webp\">\n  <p>Moreover, some pictures aren't faces, so in FER 2013+ labels we have additional classes that help us clean the dataset. In conclusion, I have worked with two different labels to see how the model and the techniques adapt to these different situations.</p>\n  <p>Furthermore it's an unbalanced problem:</p>\n  <img src=\"/imgs/data_barplot.webp\">\n</div>\n<h2 id=\"baseModel\">Choosing the base model</h2>\n<div>\n  <p>The perfect model must be fast to do as many experiments as possible and at the same time to have an acceptable precision in its classification. So I tried different configurations</p>\n</div>\n<h3 id=\"transferLearning\">Transfer learning</h3>\n<div>\n  <p>The first model decided to train is actually one already pre-trained with external images not included in the dataset we are working with. As the core of the network, the part that extracts features from the images, I chose ResNet 50. It is a residual network with 48 convolutional layers, a MaxPool layer, and an AveragePool layer. They are called residual networks because they stack residual blocks.</p>\n  <p>To adapt the ResNet 50 to our case, I have used transfer learning, which is a technique for transferring knowledge that works for one problem to another problem. The application has been to provide a new head to the ResNet and train both the head and the connections with the ResNet.</p>\n  <p>I have opted for the simplest possible head to see where we start from: a dense layer with as many neurons as there are classes, which is 7.</p>\n</div>\n<h3 id=\"fineTuning\">Fine tuning</h3>\n<div>\n  <p>Consisting of unfreezing the final phase of the base. In other words, we also train the last layers of the base with our data. This allows for better integration with the customized head we use and, therefore, makes the network's knowledge more specific to the particular problem. This technique offers much better results than just embedding the head as before.</p>\n  <p>However, it requires many more resources because a network as the 50-layer ResNet has many parameters to train, even only training the final part of the base the demand increases significantly and, as a result, the training time also increases.</p>\n  <p>I would have preferred to use the fine-tuned 50-layer ResNet throughout all the tests I conducted, but it was not feasible owing to time constraints. The tests would have taken too long, and the goal of this work is to compare differences in models before and after using the techniques mentioned above.</p>\n  <p>If we only decide to train the head from the previous chapter, which is the simplest approach, we have approximately fourteen thousand parameters to train. With fine-tuning, we can unfreeze layers to the extent that we have around twenty-three and a half million parameters.</p>\n</div>\n<h3 id=\"custom\">Customized CNN</h3>\n<div>\n  <p>In machine learning, the speed and the number of training iterations are crucial factors. To expedite the experiments, I have created a model with significantly fewer parameters than the ResNet50.</p>\n  <p>This model, in addition to having almost four and a half million trainable parameters, which are several million fewer parameters, is designed to be trained on 48x48 images, which are processed much more quickly. Therefore, the processing times with this new model are significantly shorter.</p>\n</div>\n<div>\n  <p>The perfect model must be fast to do as many experiments as possible and at the same time to have an acceptable precision in its classification. So I tried different configurations</p>\n</div>\n<h3 id=\"baseResults\">Comparing base results</h3>\n<div>\n  <table>\n    <thead>\n      <tr>\n        <th id=\"titleTable\" colspan=\"6\">Test accuracy</th>\n      </tr>\n      <tr>\n        <th colspan=\"2\">Transfer learning</th>\n        <th colspan=\"2\">Fine tuning</th>\n        <th colspan=\"2\">Custom</th>\n      </tr>\n      <tr>\n        <th>FER</th>\n        <th>FER+</th>\n        <th>FER</th>\n        <th>FER+</th>\n        <th>FER</th>\n        <th>FER+</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>54.68%</td>\n        <td>72.93%</td>\n        <td>42.1%</td>\n        <td>82.02%</td>\n        <td>51.78%</td>\n        <td>78.1%</td>\n      </tr>\n    </tbody>\n  </table>\n  <p>All models are trained for 30 epochs, except for the fine-tuned model because it takes too long. Therefore, we will work with the custom model since it is faster than the fine tuned and more accurate than the transfer learning one.</p>\n</div>\n<h2 id=\"dataAugmentation\">Data augmentation</h2>\n<div> \n  <p>The more examples our model sees, the better. But what can we do if we've already used the entire dataset? Data augmentation addresses this issue. It's a technique, as the name suggests, for adding new examples.</p>\n  <p>More or less, because in reality, we take the examples we've already used and apply small changes to make the network think they are new images without having to invest more time and money in expanding the dataset.We make the model generalize better, allowing us to train more epochs making falling into overfitting less likely, ultimately resulting in a better final model.</p>\n  <p>However, we can't apply just any changes they have to make sense for the specific problem, else we'll achieve the opposite of what we intend.</p>\n  <p>Some of the most common transformations include rotations, horizontal flips, vertical flips, changes in contrast, changes in brightness, horizontal translation, vertical translation, edge cropping, among others.</p>\n  <p>This are the results I have obtained:</p>\n  <table>\n    <thead>\n      <tr>\n        <th id=\"titleTable\" colspan=\"4\">Test accuracy FER</th>\n      </tr>\n      <tr>\n        <th>horizontal flip</th>\n        <th>shear range=5</th>\n        <th>horizontal flip and shear range=5</th>\n        <th>horizontal flip and shear range=10</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>59.6%</td>\n        <td>60.67%</td>\n        <td>57.47%</td>\n        <td>58.89%</td>\n      </tr>\n    </tbody>\n  </table>\n  <table>\n    <thead>\n      <tr>\n        <th id=\"titleTable\" colspan=\"4\">Test accuracy FER</th>\n      </tr>\n      <tr>\n        <th>horizontal flip and shear range=5</th>\n        <th>horizontal flip and shear range=10</th>\n        <th>horizontal flip, shear range=5 and brightness_range=(0.1,0.2)</th>\n        <th>horizontal flip, shear range=5 and vertical flip</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>82.83%</td>\n        <td>82.69%</td>\n        <td>36.39%</td>\n        <td>81.35%</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n<h2 id=\"labelSmoothing\">Label smoothing</h2>\n<div> \n  <p>One of the major problems with this dataset is the ambiguity of the classes to which the images belong, as we rarely have emotions that belong solely and exclusively to one of the classes we have. That's why it's logical to consider using label smoothing. To recap: label smoothing allows us to prevent the model from being overly confident in its predictions, which is precisely what we want.</p>\n  <p>Decided, let's use label smoothing, but what do we use? As with many hyperparameters in machine learning, one of the easiest ways to choose a good value is to experiment with several options.</p>\n  <table>\n    <thead>\n    <tr>\n        <th id=\"titleTable\" colspan=\"2\">FER</th>\n      </tr>\n      <tr>\n        <th>Œµ</th>\n        <th>Test accuracy</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>0.1</td>\n        <td>57.15%</td>\n      </tr>\n      <tr>\n        <td>0.2</td>\n        <td>58.94%</td>\n      </tr>\n      <tr>\n        <td>0.3</td>\n        <td>56.91%</td>\n      </tr>\n    </tbody>\n  </table>\n  <table>\n    <thead>\n    <tr>\n        <th id=\"titleTable\" colspan=\"2\">FER+</th>\n      </tr>\n      <tr>\n        <th>Œµ</th>\n        <th>Test accuracy</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>0.1</td>\n        <td>82.02%</td>\n      </tr>\n      <tr>\n        <td>0.2</td>\n        <td>83.41%</td>\n      </tr>\n      <tr>\n        <td>0.3</td>\n        <td>82.74%</td>\n      </tr>\n      <tr>\n        <td>0.4</td>\n        <td>84.07%</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n<h2 id=\"tta\">Test time augmentation</h2>\n<div> \n  <p>Building on the previous concept that allows us to train a better model, we can make mistakes on somewhat ambiguous images. Humans tend to make mistakes as well, and one way to avoid this is by forming committees of people to provide their perspective. What's better than a good model? Multiple good models.</p>\n  <p>To emulate this reasoning, what we can do is, for each image we need to classify, apply the same transformations that we used during training, but in this case during the prediction phase, once the model is already in production.</p>\n  <p>To do this, we take the incoming image and predict, in addition to the original, all its transformations independently. Then, we aggregate those results, and from there, we obtain a more reliable class than we would have predicted by simply assigning a class to the original image.</p>\n  <p>There are several ways to aggregate the predictions, with the most common being to take the average of the probabilities obtained for each image.</p>\n  <table>\n    <thead>\n    <tr>\n        <th id=\"titleTable\" colspan=\"2\">FER</th>\n      </tr>\n      <tr>\n        <th>Examples generated for each test picture</th>\n        <th>Test accuracy</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>3</td>\n        <td>59.57%</td>\n      </tr>\n      <tr>\n        <td>10</td>\n        <td>59.7%</td>\n      </tr>\n      <tr>\n        <td>15</td>\n        <td>59.99%</td>\n      </tr>\n      <tr>\n        <td>20</td>\n        <td>59.71%</td>\n      </tr>\n    </tbody>\n  </table>\n  <table>\n    <thead>\n    <tr>\n        <th id=\"titleTable\" colspan=\"2\">FER+</th>\n      </tr>\n      <tr>\n        <th>Examples generated for each test picture</th>\n        <th>Test accuracy</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>3</td>\n        <td>84.98%</td>\n      </tr>\n      <tr>\n        <td>5</td>\n        <td>85.17%</td>\n      </tr>\n      <tr>\n        <td>10</td>\n        <td>84.84%</td>\n      </tr>\n      <tr>\n        <td>20</td>\n        <td>85.27%</td>\n      </tr>\n      <tr>\n        <td>25</td>\n        <td>85.32%</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n<h2 id=\"implementation\">Implementation  üë®üèª‚Äçüíª</h2>\n<div>\n  <p>This models are made with <b>TensorFlow Keras</b>. To see the code firsthand click on the image at the top of the page.</p>\n</div>\n<link href=\"https://fonts.googleapis.com/css?family=Caveat\" rel=\"stylesheet\">\n<style>\n  h2{\n    text-align: center;\n    font-size: 6rem;\n    display: block;\n    margin: 0 auto;\n    width: 70%;   \n    line-height: 4rem; \n    font-family: \"Caveat\";\n    padding-bottom:2vh;\n    margin-bottom:4vh;\n    border-bottom: 3px rgb(var(--accent)) dashed;\n    margin-top:15vh;\n  }\n  p{\n    text-align: center;\n    font-size: 3rem;\n    display: block;\n    margin: 0 auto;\n    width: 70%;   \n    line-height: 4rem; \n    font-family: \"Caveat\";\n    margin-top:4vh;\n  }\n  b{\n    font-size: 4rem;\n    background-color: #2E2E2E;\n  }\n  #playLink{\n    text-align: center;\n    font-size: 10rem;\n    display: block;\n    margin: 0 auto;\n    width: 70%;   \n    text-decoration: none;\n    animation: hithere 2.5s infinite;\n  }\n  @keyframes hithere {\n    30% { transform: scale(1.2); }\n    40%, 60% { transform: rotate(-20deg) scale(1.2); }\n    50% { transform: rotate(20deg) scale(1.2); }\n    70% { transform: rotate(0deg) scale(1.2); }\n    100% { transform: scale(1); }\n  }\n  li{\n    text-align: left;\n    padding-left:30vw;\n    padding-top:1vh;\n  }\n  img{\n    display: block;\n    margin-left: auto;\n    margin-right: auto;\n    margin-top:2vh;\n    width: 75vw;\n  }\n  ul{\n    text-align: -webkit-center;\n    font-size: 3rem;  \n    line-height: 4rem; \n    font-family: \"Caveat\";\n    margin-top:2vh;\n    list-style-type: none;\n  }\n  a{\n    color: rgb(var(--accent));\n  }\n  .subList{\n    padding-left:40vw;\n  }\n  h3{\n    text-align: center;\n    font-size: 5rem;\n    display: block;\n    margin: 0 auto;\n    width: 50%;   \n    line-height: 4rem; \n    font-family: \"Caveat\";\n    padding-bottom:2vh;\n    margin-bottom:4vh;\n    border-bottom: 3px #990000 dashed;\n    margin-top:15vh;\n  }\n  table {\n    margin-left:auto; \n    margin-right:auto;\n    font-size: 3rem;\n    line-height: 4rem; \n    font-family: \"Caveat\";\n    margin-top:10vh;    \n  }\n  th,td{\n    border: 2px solid #990000;\n    text-align: -webkit-center;\n    width:1%;\n  }\n  .emptyTableCell{\n    border:0;\n  }\n  th{\n    font-size: 4rem;\n  }\n  #titleTable{\n    text-decoration:underline;\n  }\n  table{\n    width:70vw;\n  }\n  @media only screen and (max-width: 1200px){\n\t\tth{\n      font-size:2.5rem;\n    }\n    td{\n      font-size:2rem;\n    }\n    h2,h3{\n      font-size:5rem;\n    }\n    li{\n      padding-left:15vw;\n    }\n    ul{\n      text-align: -webkit-center;\n      font-size: 2.5rem;  \n      line-height: 4rem; \n      font-family: \"Caveat\";\n      margin-top:2vh;\n      list-style-type: none;\n    }\n    .subList{\n      padding-left:25vw;\n    }\n    p{\n      font-size: 2.5rem;\n      line-height: 4rem; \n    }\n    b{\n      font-size: 3rem;\n      background-color: #2E2E2E;\n    }\n\t}\n  @media only screen and (max-width: 600px){\n\t\tth{\n      font-size:1rem;\n    }\n    td{\n      font-size:1rem;\n    }\n    h2,h3{\n      font-size:2rem;\n    }\n    li{\n      padding-left:10vw;\n    }\n    ul{\n      text-align: -webkit-center;\n      font-size: 1rem;  \n      line-height: 2rem; \n      font-family: \"Caveat\";\n      margin-top:2vh;\n      list-style-type: none;\n    }\n    .subList{\n      padding-left:20vw;\n    }\n    p{\n      font-size: 1rem;\n      line-height: 2rem; \n    }\n    b{\n      font-size: 1.5rem;\n      background-color: #2E2E2E;\n    }\n\t}\n</style>");

				const frontmatter = {"layout":"../../layouts/ProjectsLayout.astro","title":"Final college course project","image":"/imgs/tfg.webp","transition_image":"tfg_img","transition_title":"tfg_ttl","description":"A research on simple techniques to improve a CNN applied to FER2013","github":"https://github.com/alvarolarraya/FinalCollegeCourseProject","cardNumber":1,"aiRelated":true,"hasDedicatedPage":true};
				const file = "/Users/alvar/Desktop/portfolio/src/pages/projects/03_tfg.md";
				const url = "/projects/03_tfg";
				function rawContent() {
					return "<h2 class=\"projectTitles\">Index</h2>\n<ul>\n  <li> ‚¶ø <a href=\"#generalExplanation\">What's all this about? üò∂</a></li>\n  <li> ‚¶ø <a href=\"#techniques\">Techniques analyzed</a></li>\n  <li> ‚¶ø <a href=\"#dataset\">Dataset</a></li>\n  <li> ‚¶ø <a href=\"#baseModel\">Choosing the base model</a></li>\n    <li class=\"subList\"> ‚¶æ <a href=\"#transferLearning\">Transfer Learning</a></li>\n    <li class=\"subList\"> ‚¶æ <a href=\"#fineTuning\">Fine tuning</a></li>\n    <li class=\"subList\"> ‚¶æ <a href=\"#custom\">Customized CNN</a></li>\n    <li class=\"subList\"> ‚¶æ <a href=\"#baseResults\">Comparing base results</a></li>\n  <li> ‚¶ø <a href=\"#dataAugmentation\">Data augmentation</a></li>\n  <li> ‚¶ø <a href=\"#labelSmoothing\">Label smoothing</a></li>\n  <li> ‚¶ø <a href=\"#tta\">Test time augmentation</a></li>\n  <li> ‚¶ø <a href=\"#implementation\">Implementation  üë®üèª‚Äçüíª</a></li>\n</ul>\n\n<h2 id=\"generalExplanation\">What's all this about? üò∂</h2>\n\n<div><p><b>FER 2013</b> (Face Emotion Recognition) is a dataset that offers the possibility to study a vast number of facial expressions and predict the emotion they are feeling. This project's goal is to analyze the influence of certain machine learning and computer vision techniques.</p><br><p>Starting by creating a very simple <b>CNN</b> which classifies each picture and then applying little changes that almost doesn't mutate the model's structure, yet provide a considerable improvement in the classificator understanding of the problem.</p></div>\n\n<h2 id=\"techniques\">Techniques analyzed</h2>\n\n<ul>\n  <li> ‚¶ø Transfer learning</li>\n  <li> ‚¶ø Fine tuning</li>\n  <li> ‚¶ø Data augmentation</li>\n  <li> ‚¶ø Label smoothing</li>\n  <li> ‚¶ø Test time augmentation</li>\n</ul>\n\n<h2 id=\"dataset\">Dataset</h2>\n\n<div>\n  <p>All the images are 48x48, in grayscale, the faces are centered similarly and there are seven classes: angry, disgust, fear, happy, sad, surprise and neutral. 28708 examples are to train and 7178 to test.</p>\n  <p>As you can see in the sample emotions are ambiguous due to the fact that a face can be classified as more than one emotion and still be correct. Have we got better labels, the model would be much more accurate. To solve this problem I have used besides FER 2013 labels FER 2013+ ones, whom have been made by 10 persons each one voting a class.</p>\n  <img src=\"/imgs/fer+.webp\">\n  <p>Moreover, some pictures aren't faces, so in FER 2013+ labels we have additional classes that help us clean the dataset. In conclusion, I have worked with two different labels to see how the model and the techniques adapt to these different situations.</p>\n  \n  <p>Furthermore it's an unbalanced problem:</p>\n  <img src=\"/imgs/data_barplot.webp\">\n</div>\n\n<h2 id=\"baseModel\">Choosing the base model</h2>\n\n<div>\n  <p>The perfect model must be fast to do as many experiments as possible and at the same time to have an acceptable precision in its classification. So I tried different configurations</p>\n</div>\n\n<h3 id=\"transferLearning\">Transfer learning</h2>\n\n<div>\n  <p>The first model decided to train is actually one already pre-trained with external images not included in the dataset we are working with. As the core of the network, the part that extracts features from the images, I chose ResNet 50. It is a residual network with 48 convolutional layers, a MaxPool layer, and an AveragePool layer. They are called residual networks because they stack residual blocks.</p>\n\n  <p>To adapt the ResNet 50 to our case, I have used transfer learning, which is a technique for transferring knowledge that works for one problem to another problem. The application has been to provide a new head to the ResNet and train both the head and the connections with the ResNet.</p>\n\n  <p>I have opted for the simplest possible head to see where we start from: a dense layer with as many neurons as there are classes, which is 7.</p>\n</div>\n\n<h3 id=\"fineTuning\">Fine tuning</h3>\n\n<div>\n  <p>Consisting of unfreezing the final phase of the base. In other words, we also train the last layers of the base with our data. This allows for better integration with the customized head we use and, therefore, makes the network's knowledge more specific to the particular problem. This technique offers much better results than just embedding the head as before.</p>\n\n  <p>However, it requires many more resources because a network as the 50-layer ResNet has many parameters to train, even only training the final part of the base the demand increases significantly and, as a result, the training time also increases.</p>\n\n  <p>I would have preferred to use the fine-tuned 50-layer ResNet throughout all the tests I conducted, but it was not feasible owing to time constraints. The tests would have taken too long, and the goal of this work is to compare differences in models before and after using the techniques mentioned above.</p>\n\n  <p>If we only decide to train the head from the previous chapter, which is the simplest approach, we have approximately fourteen thousand parameters to train. With fine-tuning, we can unfreeze layers to the extent that we have around twenty-three and a half million parameters.</p>\n</div>\n\n<h3 id=\"custom\">Customized CNN</h2>\n\n<div>\n  <p>In machine learning, the speed and the number of training iterations are crucial factors. To expedite the experiments, I have created a model with significantly fewer parameters than the ResNet50.</p>\n\n  <p>This model, in addition to having almost four and a half million trainable parameters, which are several million fewer parameters, is designed to be trained on 48x48 images, which are processed much more quickly. Therefore, the processing times with this new model are significantly shorter.</p>\n</div>\n\n<div>\n  <p>The perfect model must be fast to do as many experiments as possible and at the same time to have an acceptable precision in its classification. So I tried different configurations</p>\n</div>\n\n<h3 id=\"baseResults\">Comparing base results</h2>\n\n<div>\n  <table>\n    <thead>\n      <tr>\n        <th id=\"titleTable\" colspan=\"6\">Test accuracy</th>\n      </tr>\n      <tr>\n        <th colspan=\"2\">Transfer learning</th>\n        <th colspan=\"2\">Fine tuning</th>\n        <th colspan=\"2\">Custom</th>\n      </tr>\n      <tr>\n        <th>FER</th>\n        <th>FER+</th>\n        <th>FER</th>\n        <th>FER+</th>\n        <th>FER</th>\n        <th>FER+</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>54.68%</td>\n        <td>72.93%</td>\n        <td>42.1%</td>\n        <td>82.02%</td>\n        <td>51.78%</td>\n        <td>78.1%</td>\n      </tr>\n    </tbody>\n  </table>\n  <p>All models are trained for 30 epochs, except for the fine-tuned model because it takes too long. Therefore, we will work with the custom model since it is faster than the fine tuned and more accurate than the transfer learning one.</p>\n</div>\n\n<h2 id=\"dataAugmentation\">Data augmentation</h2>\n\n<div> \n  <p>The more examples our model sees, the better. But what can we do if we've already used the entire dataset? Data augmentation addresses this issue. It's a technique, as the name suggests, for adding new examples.</p>\n\n  <p>More or less, because in reality, we take the examples we've already used and apply small changes to make the network think they are new images without having to invest more time and money in expanding the dataset.We make the model generalize better, allowing us to train more epochs making falling into overfitting less likely, ultimately resulting in a better final model.</p>\n\n  <p>However, we can't apply just any changes they have to make sense for the specific problem, else we'll achieve the opposite of what we intend.</p>\n\n  <p>Some of the most common transformations include rotations, horizontal flips, vertical flips, changes in contrast, changes in brightness, horizontal translation, vertical translation, edge cropping, among others.</p>\n\n  <p>This are the results I have obtained:</p>\n\n  <table>\n    <thead>\n      <tr>\n        <th id=\"titleTable\" colspan=\"4\">Test accuracy FER</th>\n      </tr>\n      <tr>\n        <th>horizontal flip</th>\n        <th>shear range=5</th>\n        <th>horizontal flip and shear range=5</th>\n        <th>horizontal flip and shear range=10</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>59.6%</td>\n        <td>60.67%</td>\n        <td>57.47%</td>\n        <td>58.89%</td>\n      </tr>\n    </tbody>\n  </table>\n\n  <table>\n    <thead>\n      <tr>\n        <th id=\"titleTable\" colspan=\"4\">Test accuracy FER</th>\n      </tr>\n      <tr>\n        <th>horizontal flip and shear range=5</th>\n        <th>horizontal flip and shear range=10</th>\n        <th>horizontal flip, shear range=5 and brightness_range=(0.1,0.2)</th>\n        <th>horizontal flip, shear range=5 and vertical flip</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>82.83%</td>\n        <td>82.69%</td>\n        <td>36.39%</td>\n        <td>81.35%</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n\n<h2 id=\"labelSmoothing\">Label smoothing</h2>\n\n<div> \n  <p>One of the major problems with this dataset is the ambiguity of the classes to which the images belong, as we rarely have emotions that belong solely and exclusively to one of the classes we have. That's why it's logical to consider using label smoothing. To recap: label smoothing allows us to prevent the model from being overly confident in its predictions, which is precisely what we want.</p>\n\n  <p>Decided, let's use label smoothing, but what do we use? As with many hyperparameters in machine learning, one of the easiest ways to choose a good value is to experiment with several options.</p>\n\n  <table>\n    <thead>\n    <tr>\n        <th id=\"titleTable\" colspan=\"2\">FER</th>\n      </tr>\n      <tr>\n        <th>Œµ</th>\n        <th>Test accuracy</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>0.1</td>\n        <td>57.15%</td>\n      </tr>\n      <tr>\n        <td>0.2</td>\n        <td>58.94%</td>\n      </tr>\n      <tr>\n        <td>0.3</td>\n        <td>56.91%</td>\n      </tr>\n    </tbody>\n  </table>\n\n  <table>\n    <thead>\n    <tr>\n        <th id=\"titleTable\" colspan=\"2\">FER+</th>\n      </tr>\n      <tr>\n        <th>Œµ</th>\n        <th>Test accuracy</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>0.1</td>\n        <td>82.02%</td>\n      </tr>\n      <tr>\n        <td>0.2</td>\n        <td>83.41%</td>\n      </tr>\n      <tr>\n        <td>0.3</td>\n        <td>82.74%</td>\n      </tr>\n      <tr>\n        <td>0.4</td>\n        <td>84.07%</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n\n<h2 id=\"tta\">Test time augmentation</h2>\n\n<div> \n  <p>Building on the previous concept that allows us to train a better model, we can make mistakes on somewhat ambiguous images. Humans tend to make mistakes as well, and one way to avoid this is by forming committees of people to provide their perspective. What's better than a good model? Multiple good models.</p>\n\n  <p>To emulate this reasoning, what we can do is, for each image we need to classify, apply the same transformations that we used during training, but in this case during the prediction phase, once the model is already in production.</p>\n\n  <p>To do this, we take the incoming image and predict, in addition to the original, all its transformations independently. Then, we aggregate those results, and from there, we obtain a more reliable class than we would have predicted by simply assigning a class to the original image.</p>\n\n  <p>There are several ways to aggregate the predictions, with the most common being to take the average of the probabilities obtained for each image.</p>\n\n  <table>\n    <thead>\n    <tr>\n        <th id=\"titleTable\" colspan=\"2\">FER</th>\n      </tr>\n      <tr>\n        <th>Examples generated for each test picture</th>\n        <th>Test accuracy</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>3</td>\n        <td>59.57%</td>\n      </tr>\n      <tr>\n        <td>10</td>\n        <td>59.7%</td>\n      </tr>\n      <tr>\n        <td>15</td>\n        <td>59.99%</td>\n      </tr>\n      <tr>\n        <td>20</td>\n        <td>59.71%</td>\n      </tr>\n    </tbody>\n  </table>\n\n  <table>\n    <thead>\n    <tr>\n        <th id=\"titleTable\" colspan=\"2\">FER+</th>\n      </tr>\n      <tr>\n        <th>Examples generated for each test picture</th>\n        <th>Test accuracy</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>3</td>\n        <td>84.98%</td>\n      </tr>\n      <tr>\n        <td>5</td>\n        <td>85.17%</td>\n      </tr>\n      <tr>\n        <td>10</td>\n        <td>84.84%</td>\n      </tr>\n      <tr>\n        <td>20</td>\n        <td>85.27%</td>\n      </tr>\n      <tr>\n        <td>25</td>\n        <td>85.32%</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n\n<h2 id=\"implementation\">Implementation  üë®üèª‚Äçüíª</h2>\n\n<div>\n  <p>This models are made with <b>TensorFlow Keras</b>. To see the code firsthand click on the image at the top of the page.</p>\n</div>\n\n\n\n\n\n<link href='https://fonts.googleapis.com/css?family=Caveat' rel='stylesheet'>\n<style>\n  h2{\n    text-align: center;\n    font-size: 6rem;\n    display: block;\n    margin: 0 auto;\n    width: 70%;   \n    line-height: 4rem; \n    font-family: \"Caveat\";\n    padding-bottom:2vh;\n    margin-bottom:4vh;\n    border-bottom: 3px rgb(var(--accent)) dashed;\n    margin-top:15vh;\n  }\n  p{\n    text-align: center;\n    font-size: 3rem;\n    display: block;\n    margin: 0 auto;\n    width: 70%;   \n    line-height: 4rem; \n    font-family: \"Caveat\";\n    margin-top:4vh;\n  }\n  b{\n    font-size: 4rem;\n    background-color: #2E2E2E;\n  }\n  #playLink{\n    text-align: center;\n    font-size: 10rem;\n    display: block;\n    margin: 0 auto;\n    width: 70%;   \n    text-decoration: none;\n    animation: hithere 2.5s infinite;\n  }\n  @keyframes hithere {\n    30% { transform: scale(1.2); }\n    40%, 60% { transform: rotate(-20deg) scale(1.2); }\n    50% { transform: rotate(20deg) scale(1.2); }\n    70% { transform: rotate(0deg) scale(1.2); }\n    100% { transform: scale(1); }\n  }\n  li{\n    text-align: left;\n    padding-left:30vw;\n    padding-top:1vh;\n  }\n  img{\n    display: block;\n    margin-left: auto;\n    margin-right: auto;\n    margin-top:2vh;\n    width: 75vw;\n  }\n  ul{\n    text-align: -webkit-center;\n    font-size: 3rem;  \n    line-height: 4rem; \n    font-family: \"Caveat\";\n    margin-top:2vh;\n    list-style-type: none;\n  }\n  a{\n    color: rgb(var(--accent));\n  }\n  .subList{\n    padding-left:40vw;\n  }\n  h3{\n    text-align: center;\n    font-size: 5rem;\n    display: block;\n    margin: 0 auto;\n    width: 50%;   \n    line-height: 4rem; \n    font-family: \"Caveat\";\n    padding-bottom:2vh;\n    margin-bottom:4vh;\n    border-bottom: 3px #990000 dashed;\n    margin-top:15vh;\n  }\n  table {\n    margin-left:auto; \n    margin-right:auto;\n    font-size: 3rem;\n    line-height: 4rem; \n    font-family: \"Caveat\";\n    margin-top:10vh;    \n  }\n  th,td{\n    border: 2px solid #990000;\n    text-align: -webkit-center;\n    width:1%;\n  }\n  .emptyTableCell{\n    border:0;\n  }\n  th{\n    font-size: 4rem;\n  }\n  #titleTable{\n    text-decoration:underline;\n  }\n  table{\n    width:70vw;\n  }\n  @media only screen and (max-width: 1200px){\n\t\tth{\n      font-size:2.5rem;\n    }\n    td{\n      font-size:2rem;\n    }\n    h2,h3{\n      font-size:5rem;\n    }\n    li{\n      padding-left:15vw;\n    }\n    ul{\n      text-align: -webkit-center;\n      font-size: 2.5rem;  \n      line-height: 4rem; \n      font-family: \"Caveat\";\n      margin-top:2vh;\n      list-style-type: none;\n    }\n    .subList{\n      padding-left:25vw;\n    }\n    p{\n      font-size: 2.5rem;\n      line-height: 4rem; \n    }\n    b{\n      font-size: 3rem;\n      background-color: #2E2E2E;\n    }\n\t}\n  @media only screen and (max-width: 600px){\n\t\tth{\n      font-size:1rem;\n    }\n    td{\n      font-size:1rem;\n    }\n    h2,h3{\n      font-size:2rem;\n    }\n    li{\n      padding-left:10vw;\n    }\n    ul{\n      text-align: -webkit-center;\n      font-size: 1rem;  \n      line-height: 2rem; \n      font-family: \"Caveat\";\n      margin-top:2vh;\n      list-style-type: none;\n    }\n    .subList{\n      padding-left:20vw;\n    }\n    p{\n      font-size: 1rem;\n      line-height: 2rem; \n    }\n    b{\n      font-size: 1.5rem;\n      background-color: #2E2E2E;\n    }\n\t}\n</style>";
				}
				function compiledContent() {
					return html;
				}
				function getHeadings() {
					return [];
				}

				const Content = createComponent((result, _props, slots) => {
					const { layout, ...content } = frontmatter;
					content.file = file;
					content.url = url;

					return renderTemplate`${renderComponent(result, 'Layout', $$ProjectsLayout, {
								file,
								url,
								content,
								frontmatter: content,
								headings: getHeadings(),
								rawContent,
								compiledContent,
								'server:root': true,
							}, {
								'default': () => renderTemplate`${unescapeHTML(html)}`
							})}`;
				});

export { Content, compiledContent, Content as default, file, frontmatter, getHeadings, images, rawContent, url };
